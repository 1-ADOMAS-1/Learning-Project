{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "M-UWacg3eJoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import libraries"
      ],
      "metadata": {
        "id": "H9lEE381eaCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms.v2 as v2\n",
        "\n",
        "from torch.utils.data import BatchSampler, SequentialSampler\n",
        "from torchvision import models\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "7qu4bmUIQMCK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5VIlTHpPQ5k"
      },
      "outputs": [],
      "source": [
        "!pip install natasha"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from navec import Navec\n",
        "path = 'navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
        "navec = Navec.load(path)"
      ],
      "metadata": {
        "id": "WJQ3xaC9PXiU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Dataset"
      ],
      "metadata": {
        "id": "UutVsamdeXne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordsDataset(data.Dataset):\n",
        "  def __init__(self, path, navec_emb, prev_word=3):\n",
        "    self.prev_word = prev_word\n",
        "    self.navec_emb = navec_emb\n",
        "\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "      self.text = f.read()\n",
        "      self.text = self.text.replace('\\ufeff', ' ')\n",
        "      self.text = self.text.replace('\\n', ' ')\n",
        "      self.text = re.sub(r'[^А-яA-z-]', ' ', self.text)\n",
        "\n",
        "    self.words = self.text.lower().split()\n",
        "    self.words = [word for word in self.words if word in self.navec_emb]\n",
        "    vocab = set(self.words)\n",
        "\n",
        "    self.int_to_word = dict(enumerate((vocab)))\n",
        "    self.word_to_int = {v: k for k, v in self.int_to_word.items()}\n",
        "    self.vocab_size = len(vocab)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    _data = torch.vstack([torch.tensor(self.navec_emb[self.words[x]])\n",
        "                          for x in range(item, item+self.prev_word)])\n",
        "\n",
        "    word = self.words[item+self.prev_word]\n",
        "    t = self.word_to_int[word]\n",
        "    return _data, t\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.words) - 1 - self.prev_word"
      ],
      "metadata": {
        "id": "OVTsFIKJSGbk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rnn"
      ],
      "metadata": {
        "id": "BuSViS9OeU4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class wordsRNN(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super().__init__()\n",
        "    self.hidden_size = 256\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "\n",
        "    self.rnn = nn.RNN(in_features, self.hidden_size, batch_first=True)\n",
        "    self.Linear = nn.Linear(self.hidden_size, out_features)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, h = self.rnn(x)\n",
        "    y = self.Linear(h)\n",
        "    return y"
      ],
      "metadata": {
        "id": "-9Imsd7AVf8p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_train = WordsDataset('/content/text_2', navec, prev_word=3)\n",
        "train_loader = data.DataLoader(d_train, batch_size=8, shuffle=True)\n",
        "\n",
        "model = wordsRNN(300, d_train.vocab_size)"
      ],
      "metadata": {
        "id": "B-ZN1B4RWg7U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "CfoWOV0MeR8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(params=model.parameters(), lr=0.001)\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "epoch=100\n",
        "model.train()\n",
        "\n",
        "for _e in range(epoch):\n",
        "  loss_mean = 0\n",
        "  lm_count = 0\n",
        "\n",
        "  train_tqdm = tqdm(train_loader, leave=False)\n",
        "  for x_train, y_train in train_tqdm:\n",
        "    pred = model(x_train).squeeze(0)\n",
        "    loss = loss_f(pred, y_train.long())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    lm_count+=1\n",
        "    loss_mean = 1/lm_count * loss.item() + (1-1/lm_count)*loss_mean\n",
        "    train_tqdm.set_description(f'[epoch ({_e+1}/{epoch}], loss_mean: {loss_mean:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-74dXMP4YaZA",
        "outputId": "712e7a6d-4b29-48d5-8ae7-3611f4a933a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save"
      ],
      "metadata": {
        "id": "nCPRPR54ePyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "st = model.state_dict()\n",
        "torch.save(st, 'model_rnn_1.tar')"
      ],
      "metadata": {
        "id": "A1ZHuYtaaNm_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "m-7YFTx5eMax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "predict = \"Мой дядя самый\".lower().split()\n",
        "total = 10\n",
        "\n",
        "for _ in range(total):\n",
        "    _data = torch.vstack([torch.tensor(d_train.navec_emb[predict[-x]]) for x in range(d_train.prev_word, 0, -1)])\n",
        "    p = model(_data.unsqueeze(0)).squeeze(0)\n",
        "    indx = torch.argmax(p, dim=1)\n",
        "    predict.append(d_train.int_to_word[indx.item()])\n",
        "\n",
        "print(\" \".join(predict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oitJ9h7daPvb",
        "outputId": "1f2ff57e-81cc-4de2-f034-655e2ad4f236"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "мой дядя самый честных правил когда не в шутку занемог он уважать себя\n"
          ]
        }
      ]
    }
  ]
}